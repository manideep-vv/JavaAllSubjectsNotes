kAFKA IMPORTANT POINTS
======================
-- means it in windows we have batch files (.BAT) and in lunx we have .sh (shell script files) to run the commands
so in hacker rank since it will run on linux there we should use .sh files


kafka CLi notes
===================

2) start kafka server
# add bat localtions to path env variables
kafka-server-start F:\sotfwares\kafka_2.13-3.1.0\config
Kafka topics
==========================
1) TO see list of topics avialable
kafka-topics.sh --bootstrap-server localhost:9092 --list
bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

2) To create a topic with details
kafka-topics --bootstrap-server localhost:9092 --create --topic sakshiTopic --partitions 3 --replication-factor 1

##in above sakshiTopic is topic name

3) to decrie a topic ,to know how many partitions it has
3a) to describe all topics
kafka-topics --bootstrap-server localhost:9092 --describe

3b) to describe only a paticular topic
kafka-topics --bootstrap-server localhost:9092 --describe --topic sakshiTopic

kafka console producer
======================================
1) produce without keys -so that data will be evenly distributed among all the partitions of a topic
kafka-console-producer --bootstrap-server localhost:9092 --topic sakshiTopic 
#now put whatever messages u want it will goto that topic, if u enter a topic above which doesnt eists it will create that topic with default no of partitions

2)produce with keys
## if you produce with keys then hash will be calculated on that key sand it will decide the partition num
kafka-console-producer --bootstrap-server localhost:9092 --topic sakshiTopic --property parse.key=true --property key.seperator=:

kafka console consumer
======================================
1) create a consumer with group
bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic marriageTopic --group RamaGrp
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic MyTopic --group RamaGrp

kafka consumer groups
======================================
1) to list all consumers groups
kafka-consumer-groups --bootstrap-server localhost:9092 --list

2) to see all details in each grp , to know which consumer has read upto which partition
we should describe that gp-then it will tell howfar each consumers have read upto how many offses in each partition
kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group RamaGrp



command from fresco play -linux hacker rank since it is a linux you will see .sh files
whereas in windows we run .bat files instead of .sh file
if u see .sh means it it a script file
1) start zookeeper
======================
bin/zookeeper-server-start.sh config/zookeeper.properties
means this script file needs properties file as inputs


C:\kafka_2.13-3.3.1\bin\windows\zookeeper-server-start.bat C:\kafka_2.13-3.3.1\config\zookeeper.properties

2) start kafka server
=========================
bin/kafka-server-start.sh config/server.properties this script file needs server.properties

3. Create a topic
================================
means that kafka-topics.sh script file with the inputs where as --bootstrap.server and followed by inputs
command:
bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic FirstTopic --create --partitions 3 --replication-factor 1
------------------------------------------or-------------------
The below command will create a topic with name Multibrokerapplication with a replication factor of 3

bin/kafka-topics.sh --create –-bootstrap-server localhost:9092 --replication-factor 3 --partitions 1 --topic Multibrokerapplication
							


3a)Checking Topic and Partition Summary
===================================================
Using the following command, you can check the summary of partitions, topic name, replication factor and the in sync replicas.

bin/kafka-topics.sh --describe –-bootstrap-server localhost:9092 
--topic Multibrokerapplication

4. Create a producer Use command:
================================================
Command:-
bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic Multibrokerapplication
in the above Multibrokerapplication is the topic name
First Topic Now, you can start sending messages using this producer.
ex:-2
bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic MyTopic
Eg: >Hello

>Welcome to kafka

5. Create a consumer
==============================================
Open a new terminal in kafka folder and use
command:

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Multibrokerapplication --from-beginning

where Multibrokerapplication is the topic name 


create 1 node and multiple brokers
======================================
or broker_1:

broker.id=1
port=9093
log.dir=/tmp/kafka-logs-1
and for broker_2:

broker.id=2
port=9094
log.dir=/tmp/kafka-logs-2

Brokers and Parameters Configuration
i]. Make copies of config/server.properties file into two new config files config/server-one.properties and config/server-two.properties, for broker 1 and broker 2 respectively.

ii]. Edit the following parameters of config/server-one.properties.

The id of the broker must be set to a unique integer.

broker.id=1

The port to which the socket server listens on.

 listeners=PLAINTEXT://:9093
A comma-separated list of directories in which the log files are stored.

 log.dirs=/tmp/kafka-logs-1
Repeat the same configurations for config/server-two.propertieswith appropriate parameters set.
Run 3 brokers in three different terminals
=============================================
After changing parameters for the two copies of server.properties file, open three separate terminals, one for each broker and start Kafka server one by one.

Broker1
bin/kafka-server-start.sh config/server.properties
Broker2
bin/kafka-server-start.sh config/server-one.properties
Broker3
bin/kafka-server-start.sh config/server-two.properties



sample kafka consumer
=========================
  Properties props = new Properties();
  props.put("bootstrap.servers", "localhost:9092");
  props.put("group.id", "MyGroup");
  props.put("enable.auto.commit", "false");
  props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
  props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
  
  KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
  consumer.subscribe(Arrays.asList("MyTopic"));
  
  final int minBatchSize = 200;
  List<ConsumerRecord<String, String>> buffer = new ArrayList<>();
  while (true) {
      ConsumerRecords<String, String> records = consumer.poll(100);
      for (ConsumerRecord<String, String> record : records) {
          buffer.add(record);
      }
      if (buffer.size() >= minBatchSize) {
        
          consumer.commitSync();
          buffer.clear();
      }
  }
  
  sample kafka producer
  ============================
  
package tcs.fresco.play;

import java.util.Properties;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;

public class TheProducer {
    public static void main(String args[]) {
        produceMessages();
    }
    public static void produceMessages(){
        String topic = "MyTopic"; //Use this topic.
        
        Properties kafkaProps = new Properties();
kafkaProps.put("bootstrap.servers","localhost:9092");

kafkaProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
kafkaProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
 kafkaProps.put("acks","all");
      
      //If the request fails, the producer can automatically retry,
      kafkaProps.put("retries", 0);
      
            
KafkaProducer<String, String> producer = new KafkaProducer<String, String>(kafkaProps);
ProducerRecord<String, String> record = new ProducerRecord<>(topic,"Hello");
producer.send(record);
producer.close();

        
    }
} 

in this u should both compile and  run  using below commands

javac -cp "/projects/challenge/kafka/libs/*" tcs/fresco/play/TheProducer.java
java -cp .:/projects/challenge/kafka/libs/* tcs/fresco/play/TheProducer > /dev/null 0>&1 &




_configuration parameter is used to specify the time between each heartbeats to the group coordinator.
session.timeout.ms

The response obtained from broker on a metadata request is cached and the cached information is refreshed in _ 
interval.
metadata.max.age.ms

The _ is used to maintain a mapping from offsets to segment files and positions within the file.
index

The _ request is used by producer and consumer application to find the leader of each partition replica.
metadata


The _ tool provided by AdminClient API helps in operations like topic creation, modification, deletion and listing.
kafka-topics.sh

_ is the minimum amount of data needed in a topic partition for a consumer to request data from the broker
fetch.min.bytes

The _ configuration parameter specifies the maximum amount of time a replication can be delayed in replicating a new message by a replica in ISR.

replica.lag.timemax.ms

The value for ack parameter that specifies that the leader will acknowledge producer as soon as the record is written to its partition log before receiving acknowledgement from all followers.
acks=0

The _ configuration parameter specifies the maximum amount of time a replication can be delayed in replicating a new message by a replica in ISR.
replica.lag.max.time.ms

_is the configuration parameter that is used to specify the number of bytes the broker will return from each partition.
max.partition.fetch.bytes

_ is the minimum amount of data needed in a topic partition for a consumer to request data from the broker
fetch.minimum.bytes

Synchronous send() method uses Future.get() method which will wait to get a reply from Broker. If record is sent successfully get() method will get _ object which is used to get offset of the message written to broker.
callback


The _ is the replica that was the leader when that topic was originally created
leader

_ is the first broker that starts in a cluster and responsible for electing leader for a partition .
controller

When a consumer in a consumer group fails, the partitions it was assigned with will be transferred to another consumer in the consumer group which is called a _ ?
partition rebalance

The _ threads are responsible for picking up and processing requests placed in a request queue by processor thread.
From the Request Queue, the I/O thread picks up the request, processes them and places them in a Response Queue.

The value to be set to ack to specify the leader should wait till it receive acknowledgement from the full set of In Sync replicas.
acks=all

The follower replicas are used only for replication purpose and do not serve client requests

true
In order to delete a message completely from the segment, the producer produces the same message again with its key having NULL value.
false

The heartbeats are send to the group coordinator when it retrieves record from Kafka broker using _ method
poll

The configuration parameter to specify the log clean up policy in Kafka Brokers.
log.cleanup.policy=delete

_ is the minimum amount of data needed in a topic partition for a consumer to request data from the broker
fetch.min.bytes







